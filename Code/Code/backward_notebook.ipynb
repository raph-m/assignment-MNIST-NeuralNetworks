{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from read_dataset import read_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from displayData import displayData\n",
    "from randInitializeWeights import randInitializeWeights\n",
    "from unroll_params import unroll_params\n",
    "from roll_params import roll_params\n",
    "from scipy.optimize import *\n",
    "from predict import predict\n",
    "from sigmoid import sigmoid\n",
    "from sigmoidGradient import sigmoidGradient\n",
    "from debugInializeWeights import debugInitializeWeights\n",
    "from costFunction import costFunction\n",
    "from unroll_params import unroll_params\n",
    "from computeNumericalGradient import computeNumericalGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading of the dataset\n",
    "# You are free to reduce the number of samples retained for training, in order to reduce the computational cost\n",
    "size_training = 60000     # number of samples retained for training\n",
    "size_test     = 10000     # number of samples retained for testing\n",
    "images_training, labels_training, images_test, labels_test = read_dataset(size_training, size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkNNGradients(lambd):\n",
    "\n",
    "    input_layer_size  = 3;\n",
    "    hidden_layer_size = 5;\n",
    "    num_labels = 2;\n",
    "    m          = 10;\n",
    "    layers     = [3, 5, 2]\n",
    "\n",
    "    # In this point we generate a number of random data\n",
    "    Theta = [] \n",
    "    Theta.append(debugInitializeWeights(hidden_layer_size, input_layer_size))\n",
    "    Theta.append(debugInitializeWeights(num_labels, hidden_layer_size))\n",
    "\n",
    "    X = debugInitializeWeights(m, input_layer_size - 1)\n",
    "    y = np.remainder(np.arange(m)+1, num_labels)\n",
    "    \n",
    "    # Unroll parameters\n",
    "    nn_params = unroll_params(Theta)\n",
    "\n",
    "    # Compute Numerical Gradient\n",
    "    numgrad = computeNumericalGradient(nn_params,layers, X, y, num_labels, lambd)\n",
    "\n",
    "    # Compute Analytical Gradient (BackPropagation)\n",
    "    truegrad = backwards(nn_params, layers, X, y, num_labels, lambd)\n",
    "\n",
    "    \n",
    "    print(np.concatenate(([numgrad], [truegrad]), axis = 0).transpose())\n",
    "    print(\"The above two columns must be very similar.\\n(Left-Numerical Gradient, Right-Analytical Gradient (BackPropagation)\\n\")\n",
    "    \n",
    "    diff = np.linalg.norm(numgrad - truegrad) / np.linalg.norm(numgrad + truegrad)\n",
    "    print(\"\\nNote: If the implementation of the backpropagation is correct, the relative different must be quite small (less that 1e-09).\")\n",
    "    print(\"Relative difference: \" + str(diff) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertOne(x):\n",
    "    print(\"INSERTONE\")\n",
    "    print(x.shape)\n",
    "    s = x.shape\n",
    "    a = np.ones((s[0], s[1]+1))\n",
    "    a[:, 1:] = x\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwards(nn_weights, layers, X, y, num_labels, lambd):\n",
    "    # Computes the gradient fo the neural network.\n",
    "    # nn_weights: Neural network parameters (vector)\n",
    "    # layers: a list with the number of units per layer.\n",
    "    # X: a matrix where every row is a training example for a handwritten digit image\n",
    "    # y: a vector with the labels of each instance\n",
    "    # num_labels: the number of units in the output layer\n",
    "    # lambd: regularization factor\n",
    "    \n",
    "    # Setup some useful variables\n",
    "    m = X.shape[0]\n",
    "    num_layers = len(layers)\n",
    "\n",
    "    # Roll Params\n",
    "    # The parameters for the neural network are \"unrolled\" into the vector\n",
    "    # nn_params and need to be converted back into the weight matrices.\n",
    "    Theta = roll_params(nn_weights, layers)\n",
    "    \n",
    "    print(\"np.shape(Theta[0]):\")\n",
    "    print(np.shape(Theta[0]))\n",
    "    print(\"np.shape(Theta[1]):\")\n",
    "    print(np.shape(Theta[1]))\n",
    "    # ================================ TODO ================================\n",
    "    # The vector y passed into the function is a vector of labels\n",
    "    # containing values from 1..K. You need to map this vector into a \n",
    "    # binary vector of 1's and 0's to be used with the neural network\n",
    "    # cost function.\n",
    "    yv = np.zeros((num_labels, m))\n",
    "    for i in range(len(y)):\n",
    "        yv[int(y[i])] = 1  # TODO: the int conversion is maybe not the useful\n",
    "    yv = np.transpose(yv)\n",
    "\n",
    "    a = []\n",
    "    z = []\n",
    "    x = np.copy(X)\n",
    "    a.append(insertOne(x))\n",
    "    z.append(x)\n",
    "\n",
    "\n",
    "    for i in range(num_layers - 1):\n",
    "        print(\"shape of x at stage \"+str(i))\n",
    "        print(np.shape(x))\n",
    "\n",
    "        s = np.shape(Theta[i])\n",
    "        theta = Theta[i][:, 1:s[1]]\n",
    "        x = np.dot(x, np.transpose(theta))\n",
    "        x = x + Theta[i][:, 0]\n",
    "        z.append(x)\n",
    "        x = sigmoid(x)\n",
    "        a.append(insertOne(x))\n",
    "        \n",
    "    print(\"shape of x at the end \")\n",
    "    print(np.shape(x))\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    delta = [np.zeros(w.shape) for w in z]\n",
    "    delta[num_layers-1] = (x-yv)\n",
    "    \n",
    "    print(\"np.shape(delta[num_layers-1]: )\")\n",
    "    print(np.shape(delta[num_layers-1]))\n",
    "    \n",
    "    for i in range(num_layers-2, 0, -1):\n",
    "        print(\"computing delta for i=\"+str(i))\n",
    "        s = np.shape(Theta[i])\n",
    "        theta = Theta[i][:, 1:s[1]]\n",
    "        temp = np.dot(np.transpose(theta), np.transpose(delta[i+1]))\n",
    "        delta[i] = np.transpose(temp)*sigmoidGradient(z[i])\n",
    "        print(\"np.shape(delta[i]): \")\n",
    "        print(np.shape(delta[i]))\n",
    "\n",
    "    Delta = []\n",
    "    for i in range(num_layers-1):\n",
    "        temp = np.dot(np.transpose(delta[i+1]), a[i])\n",
    "        print(temp)\n",
    "        Delta.append(temp)\n",
    "    \n",
    "    for d in Delta:\n",
    "        print(d)\n",
    "    \n",
    "    cost = (yv * np.log(x) - (1 - yv) * np.log(1 - x)) / m\n",
    "    cost = -np.sum(cost)\n",
    "\n",
    "    somme = 0\n",
    "\n",
    "    for i in range(num_layers - 1):\n",
    "        somme += lambd * np.sum(Theta[i] ** 2) / (2 * m)\n",
    "\n",
    "    cost += somme\n",
    "\n",
    "    # ================================ TODO ================================\n",
    "    # In this point implement the backpropagaition algorithm \n",
    "    Theta_grad = [d/m for d in Delta]\n",
    "    # Unroll Params\n",
    "    Theta_grad = unroll_params(Theta_grad)\n",
    "    return Theta_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.shape(Theta[0]):\n",
      "(5, 4)\n",
      "np.shape(Theta[1]):\n",
      "(2, 6)\n",
      "INSERTONE\n",
      "(10, 3)\n",
      "shape of x at stage 0\n",
      "(10, 3)\n",
      "INSERTONE\n",
      "(10, 5)\n",
      "shape of x at stage 1\n",
      "(10, 5)\n",
      "INSERTONE\n",
      "(10, 2)\n",
      "shape of x at the end \n",
      "(10, 2)\n",
      "np.shape(delta[num_layers-1]: )\n",
      "(10, 2)\n",
      "computing delta for i=1\n",
      "np.shape(delta[i]): \n",
      "(10, 5)\n",
      "[[ -2.32773805e-01  -1.41648590e-03  -3.12686920e-04   1.07859497e-03]\n",
      " [ -6.78272983e-02  -4.14101838e-04  -9.43728661e-05   3.12122084e-04]\n",
      " [  1.59654070e-01   9.74386868e-04   2.18584503e-04  -7.38183446e-04]\n",
      " [  2.40476843e-01   1.46302085e-03   3.19326390e-04  -1.11795527e-03]\n",
      " [  1.00018542e-01   6.09263602e-04   1.36214867e-04  -4.62069189e-04]]\n",
      "[[-4.90656507 -2.55757089 -2.33508856 -2.50355949 -2.50582419 -2.33439176]\n",
      " [-4.91869395 -2.56389308 -2.34086112 -2.50974787 -2.51201866 -2.34016245]]\n",
      "[[ -2.32773805e-01  -1.41648590e-03  -3.12686920e-04   1.07859497e-03]\n",
      " [ -6.78272983e-02  -4.14101838e-04  -9.43728661e-05   3.12122084e-04]\n",
      " [  1.59654070e-01   9.74386868e-04   2.18584503e-04  -7.38183446e-04]\n",
      " [  2.40476843e-01   1.46302085e-03   3.19326390e-04  -1.11795527e-03]\n",
      " [  1.00018542e-01   6.09263602e-04   1.36214867e-04  -4.62069189e-04]]\n",
      "[[-4.90656507 -2.55757089 -2.33508856 -2.50355949 -2.50582419 -2.33439176]\n",
      " [-4.91869395 -2.56389308 -2.34086112 -2.50974787 -2.51201866 -2.34016245]]\n",
      "[[ -1.15683612e-04  -2.32773805e-02]\n",
      " [ -2.61687916e-05  -1.41648590e-04]\n",
      " [  8.74054951e-05  -3.12686920e-05]\n",
      " [ -1.89297862e-02   1.07859497e-04]\n",
      " [ -1.46315408e-04  -6.78272983e-03]\n",
      " [ -3.23508376e-05  -4.14101838e-05]\n",
      " [  1.11356943e-04  -9.43728661e-06]\n",
      " [ -2.39710744e-02   3.12122084e-05]\n",
      " [ -4.26300462e-05   1.59654070e-02]\n",
      " [ -9.23299659e-06   9.74386868e-05]\n",
      " [  3.26528282e-05   2.18584503e-05]\n",
      " [ -7.00337318e-03  -7.38183446e-05]\n",
      " [  1.00339516e-04   2.40476843e-02]\n",
      " [  2.23733521e-05   1.46302085e-04]\n",
      " [ -7.61627705e-05   3.19326390e-05]\n",
      " [  1.64512032e-02  -1.11795527e-04]\n",
      " [  1.51218511e-04   1.00018542e-02]\n",
      " [  3.41262940e-05   6.09263602e-05]\n",
      " [ -1.14341477e-04   1.36214867e-05]\n",
      " [  2.47389954e-02  -4.62069189e-05]\n",
      " [ -2.43057965e-01  -4.90656507e-01]\n",
      " [ -2.64902842e-01  -2.55757089e-01]\n",
      " [ -2.45845000e-01  -2.33508856e-01]\n",
      " [ -2.48913432e-01  -2.50355949e-01]\n",
      " [ -2.63961453e-01  -2.50582419e-01]\n",
      " [ -5.05074702e-01  -2.33439176e-01]\n",
      " [ -2.44124190e-01  -4.91869395e-01]\n",
      " [ -2.66064863e-01  -2.56389308e-01]\n",
      " [ -2.46923397e-01  -2.34086112e-01]\n",
      " [ -2.50005350e-01  -2.50974787e-01]\n",
      " [ -2.65119325e-01  -2.51201866e-01]\n",
      " [ -5.07290261e-01  -2.34016245e-01]]\n",
      "The above two columns must be very similar.\n",
      "(Left-Numerical Gradient, Right-Analytical Gradient (BackPropagation)\n",
      "\n",
      "\n",
      "Note: If the implementation of the backpropagation is correct, the relative different must be quite small (less that 1e-09).\n",
      "Relative difference: 0.255608794944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lambd = 0.0\n",
    "checkNNGradients(lambd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(10, -1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(10,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for a in range(10, 0, -1):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "range object index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b12a5639ea5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: range object index out of range"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print(a[0])\n",
    "print(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}